# Perceptron Model Project

This project explores the **Perceptron algorithm** â€” one of the simplest types of neural networks â€” and demonstrates how to process data, train a perceptron, tune its parameters, and visualize results using **Streamlit**.

---

## ğŸ”¹ What is a Perceptron?

A perceptron is a **binary classifier** that maps an input vector **x** into a binary output using a **linear prediction function**.
It consists of just one neuron:

1. **Inputs & Weights**

   * Each input has an associated **weight** that determines its importance.
   * Weights are adjusted during training to minimize errors.

2. **Bias**

   * A constant offset that shifts the decision boundary (e.g., moves a separating line up/down).

3. **Weighted Sum**

   * The perceptron computes:

     $$
     net = w_0x_0 + w_1x_1 + \dots + w_nx_n
     $$

4. **Activation Function**

   * Applies the weighted sum + bias, compares to a threshold, and outputs **0 or 1**.

5. **Learning Rule**

   * The perceptron updates its weights based on **prediction errors**, improving decision boundaries over time.

---

## ğŸ”¹ Project Goals

* Understand the **full pipeline**: data preprocessing â†’ model training â†’ evaluation â†’ tuning.
* Provide a **Streamlit app** so users can upload their own datasets and experiment with perceptrons interactively.
* Make the workflow modular, organized, and reusable.

---

## ğŸ”¹ Data

* Users can **upload their own dataset** (CSV or Excel).
* If no dataset is uploaded, the app falls back to a **default dataset**.

---

## ğŸ”¹ Code Structure

This project is written in a **modular style** â€” each file has a clear responsibility:

* **`data_loader.py`** â†’ Loads uploaded datasets or provides a default dataset.
* **`data_cleaner.py`** â†’ Cleans data (drops missing values). Label Encoding & standardization are applied in `main_app.py`.
* **`train_test_split.py`** â†’ Prepares features/targets and splits into train/test sets.
* **`perceptron.py`** â†’ Creates and trains the perceptron model.
* **`model_evaluation.py`** â†’ Evaluates model accuracy, precision, recall, and more.
* **`hyperparameter_tuning.py`** â†’ Runs parameter search (learning rate, iterations) and retrains model.
* **`visualization.py`** â†’ Generates plots (confusion matrix, ROC curve, feature importance, etc.).
* **`main_app.py`** â†’ The **Streamlit app**, tying everything together with an interactive UI.

---

## ğŸ”¹ Features of the App

* âœ”ï¸ Upload any dataset and train a perceptron model
* âœ”ï¸ Automatic preprocessing (cleaning, encoding, splitting)
* âœ”ï¸ Adjustable hyperparameters (iterations, learning rate)
* âœ”ï¸ Hyperparameter search for optimal settings
* âœ”ï¸ Visual analysis (confusion matrix, ROC curve, feature importance)
* âœ”ï¸ Easy-to-use **web interface with Streamlit**

Great question ğŸ‘ â€” for a GitHub README, citations should go in a **dedicated section at the end** so they donâ€™t clutter your project explanation but are still visible and properly credit the dataset authors.

Iâ€™d suggest adding a **â€œğŸ“– References / Citationâ€** section after your existing content. Example:

---

## ğŸ“– References

**Dataset**

* Rice (Cammeo and Osmancik) Dataset

  * Source: [Murat Koklu Datasets](https://www.muratkoklu.com/datasets/) | [UCI Repository](https://archive.ics.uci.edu/dataset/545/rice+cammeo+and+osmancik)
  * Authors:

    * Ä°lKay Cinar (Selcuk University, Konya, Turkey)
    * Murat Koklu (Selcuk University, Konya, Turkey)
  * Abstract: 3810 rice grains were imaged and 7 morphological features were extracted for classification.

**Citation**
Cinar, I. and Koklu, M. (2019). *Classification of Rice Varieties Using Artificial Intelligence Methods.* International Journal of Intelligent Systems and Applications in Engineering, vol.7, no.3 (Sep. 2019), pp.188â€“194. [https://doi.org/10.18201/ijisae.2019355381](https://doi.org/10.18201/ijisae.2019355381)
